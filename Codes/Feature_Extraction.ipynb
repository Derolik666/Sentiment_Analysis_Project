{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Using cached https://files.pythonhosted.org/packages/60/f0/1d9bfcc8ee6b83472ec571406bd0dd51c0e6330ff1a51b2d29861d389e85/textblob-0.15.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: nltk>=3.1 in d:\\python3.7\\lib\\site-packages (from textblob) (3.2.2)\n",
      "Requirement already satisfied: six in d:\\python3.7\\lib\\site-packages (from nltk>=3.1->textblob) (1.11.0)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.15.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.2, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "!{sys.executable} -m pip install textblob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import timeit\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.dates as md\n",
    "%matplotlib inline\n",
    "import operator \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import sentiment\n",
    "# from autocorrect import Speller # For spelling correction\n",
    "# from autocorrect import spell\n",
    "from urllib import request\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_pos = r'https://raw.githubusercontent.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107/master/data/opinion-lexicon-English/positive-words.txt'\n",
    "\n",
    "url_neg = r'https://raw.githubusercontent.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107/master/data/opinion-lexicon-English/negative-words.txt'\n",
    "\n",
    "pos_list = request.urlopen(url_pos).read().decode('utf-8')[1:]\n",
    "pos_list = pos_list[pos_list.find(\"a+\"):].split(\"\\n\")\n",
    "\n",
    "neg_list = request.urlopen(url_neg).read().decode('ISO-8859-1')[1:]\n",
    "neg_list = neg_list[neg_list.find(\"2-faced\"):].split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>Votes</th>\n",
       "      <th>id_col</th>\n",
       "      <th>Label</th>\n",
       "      <th>id_new_col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Very pleased</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Product    Brand   Price  Rating  \\\n",
       "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  Samsung  199.99       5   \n",
       "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  Samsung  199.99       4   \n",
       "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  Samsung  199.99       5   \n",
       "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  Samsung  199.99       4   \n",
       "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...  Samsung  199.99       4   \n",
       "\n",
       "                                              Review  Votes  id_col  Label  \\\n",
       "0  I feel so LUCKY to have found this used (phone...    1.0       0      1   \n",
       "1  nice phone, nice up grade from my pantach revu...    0.0       1      1   \n",
       "2                                       Very pleased    0.0       2      1   \n",
       "3  It works good but it goes slow sometimes but i...    0.0       3      1   \n",
       "4  Great phone to replace my lost phone. The only...    0.0       4      1   \n",
       "\n",
       "   id_new_col  \n",
       "0           0  \n",
       "1           1  \n",
       "2           2  \n",
       "3           3  \n",
       "4           4  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../reference/Amazon_Unlocked_Mobile.csv', delimiter = \",\")\n",
    "n = len(df)\n",
    "df.columns = ['Product', 'Brand', 'Price', 'Rating', 'Review', 'Votes']\n",
    "df['id_col'] = range(0, n)\n",
    "df['Label'] = 0\n",
    "df.loc[df['Rating'] > 3, 'Label'] = 1\n",
    "df.loc[df['Rating'] < 3, 'Label'] = -1\n",
    "# n_reviews = 100 # Let's get a sample\n",
    "# keep = sorted(random.sample(range(1,n),n_reviews))\n",
    "# keep += list(set(test.review_id)) # this are the reviews annotated for test\n",
    "\n",
    "# df = df[df.id_col.isin(keep)]\n",
    "n_reviews = len(df)\n",
    "df['id_new_col'] = range(0, n_reviews)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413840"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nokia Asha 302 Unlocked GSM Phone with 3.2MP Camera, Video, QWERTYDependableTraditional Nokia Menu'sNot Complicated like 'Smart Phones'DurableEasy to use on Straighttalk, Internet, WiFi, Bluetooth.\n"
     ]
    }
   ],
   "source": [
    "id_prod = 69\n",
    "\n",
    "for val in df[df.id_new_col == id_prod].Review:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(df, stem = False, negation = False):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop = set(stopwords.words('english'))\n",
    "    reviews = []  \n",
    "    i = 1\n",
    "    \n",
    "    for review in df[\"Review\"]:\n",
    "        tokenized_review = []      \n",
    "\n",
    "        review = str(review).lower() # lowercase\n",
    "        \n",
    "        # Remove every character except A-Z, a-z,space \n",
    "        # and punctuation (we'll need it for negation)\n",
    "        review = re.sub(r'[^A-Za-z /.]','',review) \n",
    "        \n",
    "        # mark_negation needs punctuation separated by white space.\n",
    "        review = review.replace(\".\", \" .\")   \n",
    "        \n",
    "        tokens = word_tokenize(review)\n",
    "        \n",
    "        \n",
    "        for token in tokens:\n",
    "            # Remove single characters and stop words\n",
    "            if (len(token)>1 or token == \".\") and token not in stop: \n",
    "                if stem:\n",
    "                    tokenized_review.append(stemmer.stem(get_synonym(token)))            \n",
    "                else:\n",
    "                    tokenized_review.append(get_synonym(token))\n",
    "        \n",
    "        if negation:\n",
    "            tokenized_review = sentiment.util.mark_negation(tokenized_review)   \n",
    "        \n",
    "        # Now we can get rid of punctuation and also let's fix some spellings:\n",
    "        tokenized_review = [correction(x) for x in tokenized_review if x != \".\" ]\n",
    "        \n",
    "            \n",
    "        reviews.append(tokenized_review)\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print('progress: ', (i/len(df[\"Review\"]))*100, \"%\")\n",
    "        i = i + 1\n",
    "        \n",
    "    return reviews\n",
    " \n",
    "\n",
    "def get_pos(tokenized_reviews):\n",
    "    tokenized_pos = []\n",
    "    \n",
    "    for review in tokenized_reviews:\n",
    "        tokenized_pos.append(nltk.pos_tag(review))\n",
    "    \n",
    "    return tokenized_pos\n",
    "        \n",
    "    \n",
    "def get_frequency(tokens):    \n",
    "    term_freqs = defaultdict(int)    \n",
    "    \n",
    "    for token in tokens:\n",
    "        term_freqs[token] += 1 \n",
    "            \n",
    "    return term_freqs\n",
    "\n",
    "\n",
    "def get_tdm(tokenized_reviews):\n",
    "    tdm = []\n",
    "    \n",
    "    for tokens in tokenized_reviews:\n",
    "        tdm.append(get_frequency(tokens))\n",
    "    \n",
    "    return tdm\n",
    "\n",
    "def normalize_tdm(tdm):    \n",
    "    tdm_normalized = []\n",
    "        \n",
    "    for review in tdm:\n",
    "        den = 0\n",
    "        review_normalized = defaultdict(int)\n",
    "        \n",
    "        for k,v in review.items():\n",
    "            den += v**2\n",
    "        den = math.sqrt(den)\n",
    "    \n",
    "        for k,v in review.items():\n",
    "            review_normalized[k] = v/den\n",
    "        \n",
    "        tdm_normalized.append(review_normalized)\n",
    "        \n",
    "    return tdm_normalized\n",
    "\n",
    "def get_all_terms(tokenized_reviews):\n",
    "    all_terms = []\n",
    "    \n",
    "    for tokens in tokenized_reviews:\n",
    "        for token in tokens:\n",
    "            all_terms.append(token)\n",
    "            \n",
    "    return(set(all_terms))\n",
    "    \n",
    "def get_all_terms_dft(tokenized_reviews, all_terms):\n",
    "    terms_dft = defaultdict(int)  \n",
    "    \n",
    "    for term in all_terms: \n",
    "        for review in tokenized_reviews:\n",
    "            if term in review:\n",
    "                terms_dft[term] += 1\n",
    "                \n",
    "    return terms_dft\n",
    "\n",
    "\n",
    "def get_tf_idf_transform(tokenized_reviews, tdm, n_reviews):\n",
    "    tf_idf = []        \n",
    "    all_terms = get_all_terms(tokenized_reviews)    \n",
    "    terms_dft = get_all_terms_dft(tokenized_reviews, all_terms)\n",
    "    \n",
    "    for review in tdm:\n",
    "        review_tf_idf = defaultdict(int)\n",
    "        for k,v in review.items():\n",
    "            review_tf_idf[k] = v * math.log(n_reviews / terms_dft[k], 2)\n",
    "        \n",
    "        tf_idf.append(review_tf_idf)     \n",
    "    \n",
    "    return tf_idf\n",
    "\n",
    "\n",
    "def get_idf_transform(tokenized_reviews, tdm, n_reviews):\n",
    "    idf = []    \n",
    "    terms_dft = defaultdict(int)    \n",
    "    \n",
    "    all_terms = get_all_terms(tokenized_reviews)\n",
    "    \n",
    "    for term in all_terms: \n",
    "        for review in tokenized_reviews:\n",
    "            if term in review:\n",
    "                terms_dft[term] += 1\n",
    "    \n",
    "    for review in tdm:\n",
    "        review_idf = defaultdict(int)\n",
    "        for k,v in review.items():\n",
    "            review_idf[k] = math.log(n_reviews / terms_dft[k], 2)\n",
    "        \n",
    "        idf.append(review_idf)     \n",
    "    \n",
    "    return idf\n",
    "\n",
    "\n",
    "def correction(x):\n",
    "    ok_words = [\"microsd\"]\n",
    "    \n",
    "    if x.find(\"_NEG\") == -1 and x not in ok_words: # Don't correct if they are negated words or exceptions\n",
    "        b = TextBlob(x) \n",
    "#         spell = Speller(lang='en')\n",
    "        return str(b.correct())\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def get_synonym(word):\n",
    "    synonyms = [[\"camera\",\"video\", \"display\"], \n",
    "                [\"phone\", \"cellphone\", \"smartphone\", \"phones\"],\n",
    "               [\"setting\", \"settings\"],\n",
    "               [\"feature\", \"features\"],\n",
    "               [\"pictures\", \"photos\"],\n",
    "               [\"speakers\", \"speaker\"]]\n",
    "    synonyms_parent = [\"camera\", \"phone\", \"settings\", \"features\", \"photos\", \"speakers\"]\n",
    "    \n",
    "    for i in range(len(synonyms)):\n",
    "        if word in synonyms[i]:\n",
    "            return synonyms_parent[i]\n",
    "    \n",
    "    return word\n",
    "\n",
    "\n",
    "def get_similarity_matrix(similarity, tokenized_reviews):\n",
    "    similarity_matrix = []\n",
    "    all_terms = get_all_terms(tokenized_reviews)\n",
    "    \n",
    "    for review in similarity:\n",
    "        similarity_matrix_row = []\n",
    "        for term in all_terms:\n",
    "            similarity_matrix_row.append(review[term])\n",
    "            \n",
    "        similarity_matrix.append(similarity_matrix_row)\n",
    "            \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress:  0.02416392808815001 %\n",
      "progress:  0.04832785617630002 %\n",
      "progress:  0.07249178426445002 %\n",
      "progress:  0.09665571235260004 %\n",
      "progress:  0.12081964044075005 %\n",
      "progress:  0.14498356852890004 %\n",
      "progress:  0.16914749661705006 %\n",
      "progress:  0.19331142470520007 %\n",
      "progress:  0.2174753527933501 %\n",
      "progress:  0.2416392808815001 %\n",
      "progress:  0.2658032089696501 %\n",
      "progress:  0.2899671370578001 %\n",
      "progress:  0.31413106514595013 %\n",
      "progress:  0.3382949932341001 %\n",
      "progress:  0.36245892132225016 %\n",
      "progress:  0.38662284941040015 %\n",
      "progress:  0.41078677749855014 %\n",
      "progress:  0.4349507055867002 %\n",
      "progress:  0.45911463367485017 %\n",
      "progress:  0.4832785617630002 %\n",
      "progress:  0.5074424898511503 %\n",
      "progress:  0.5316064179393002 %\n",
      "progress:  0.5557703460274502 %\n",
      "progress:  0.5799342741156002 %\n",
      "progress:  0.6040982022037502 %\n",
      "progress:  0.6282621302919003 %\n",
      "progress:  0.6524260583800502 %\n",
      "progress:  0.6765899864682002 %\n",
      "progress:  0.7007539145563503 %\n",
      "progress:  0.7249178426445003 %\n",
      "progress:  0.7490817707326504 %\n",
      "progress:  0.7732456988208003 %\n",
      "progress:  0.7974096269089502 %\n",
      "progress:  0.8215735549971003 %\n",
      "progress:  0.8457374830852503 %\n",
      "progress:  0.8699014111734004 %\n",
      "progress:  0.8940653392615504 %\n",
      "progress:  0.9182292673497003 %\n",
      "progress:  0.9423931954378504 %\n",
      "progress:  0.9665571235260004 %\n",
      "progress:  0.9907210516141505 %\n",
      "progress:  1.0148849797023005 %\n",
      "progress:  1.0390489077904503 %\n",
      "progress:  1.0632128358786004 %\n",
      "progress:  1.0873767639667504 %\n",
      "progress:  1.1115406920549005 %\n",
      "progress:  1.1357046201430505 %\n",
      "progress:  1.1598685482312003 %\n",
      "progress:  1.1840324763193504 %\n",
      "progress:  1.2081964044075004 %\n",
      "progress:  1.2323603324956505 %\n",
      "progress:  1.2565242605838005 %\n",
      "progress:  1.2806881886719506 %\n",
      "progress:  1.3048521167601004 %\n",
      "progress:  1.3290160448482504 %\n",
      "progress:  1.3531799729364005 %\n",
      "progress:  1.3773439010245505 %\n",
      "progress:  1.4015078291127006 %\n",
      "progress:  1.4256717572008506 %\n",
      "progress:  1.4498356852890006 %\n",
      "progress:  1.4739996133771507 %\n",
      "progress:  1.4981635414653007 %\n",
      "progress:  1.5223274695534508 %\n",
      "progress:  1.5464913976416006 %\n",
      "progress:  1.5706553257297509 %\n",
      "progress:  1.5948192538179005 %\n",
      "progress:  1.6189831819060505 %\n",
      "progress:  1.6431471099942005 %\n",
      "progress:  1.6673110380823506 %\n",
      "progress:  1.6914749661705006 %\n",
      "progress:  1.7156388942586507 %\n",
      "progress:  1.7398028223468007 %\n",
      "progress:  1.7639667504349508 %\n",
      "progress:  1.7881306785231008 %\n",
      "progress:  1.8122946066112509 %\n",
      "progress:  1.8364585346994007 %\n",
      "progress:  1.8606224627875507 %\n",
      "progress:  1.8847863908757008 %\n",
      "progress:  1.9089503189638508 %\n",
      "progress:  1.9331142470520009 %\n",
      "progress:  1.957278175140151 %\n",
      "progress:  1.981442103228301 %\n",
      "progress:  2.005606031316451 %\n",
      "progress:  2.029769959404601 %\n",
      "progress:  2.053933887492751 %\n",
      "progress:  2.0780978155809007 %\n",
      "progress:  2.1022617436690507 %\n",
      "progress:  2.1264256717572008 %\n",
      "progress:  2.150589599845351 %\n",
      "progress:  2.174753527933501 %\n",
      "progress:  2.198917456021651 %\n",
      "progress:  2.223081384109801 %\n",
      "progress:  2.247245312197951 %\n",
      "progress:  2.271409240286101 %\n",
      "progress:  2.295573168374251 %\n",
      "progress:  2.3197370964624007 %\n",
      "progress:  2.3439010245505507 %\n",
      "progress:  2.3680649526387008 %\n",
      "progress:  2.392228880726851 %\n",
      "progress:  2.416392808815001 %\n",
      "progress:  2.440556736903151 %\n",
      "progress:  2.464720664991301 %\n",
      "progress:  2.488884593079451 %\n",
      "progress:  2.513048521167601 %\n",
      "progress:  2.537212449255751 %\n",
      "progress:  2.561376377343901 %\n",
      "progress:  2.5855403054320507 %\n",
      "progress:  2.6097042335202008 %\n",
      "progress:  2.633868161608351 %\n",
      "progress:  2.658032089696501 %\n",
      "progress:  2.682196017784651 %\n",
      "progress:  2.706359945872801 %\n",
      "progress:  2.730523873960951 %\n",
      "progress:  2.754687802049101 %\n",
      "progress:  2.778851730137251 %\n",
      "progress:  2.803015658225401 %\n",
      "progress:  2.827179586313551 %\n",
      "progress:  2.851343514401701 %\n",
      "progress:  2.8755074424898512 %\n",
      "progress:  2.8996713705780013 %\n",
      "progress:  2.9238352986661513 %\n",
      "progress:  2.9479992267543014 %\n",
      "progress:  2.9721631548424514 %\n",
      "progress:  2.9963270829306015 %\n",
      "progress:  3.0204910110187515 %\n",
      "progress:  3.0446549391069015 %\n",
      "progress:  3.068818867195051 %\n",
      "progress:  3.092982795283201 %\n",
      "progress:  3.1171467233713512 %\n",
      "progress:  3.1413106514595017 %\n",
      "progress:  3.1654745795476518 %\n",
      "progress:  3.189638507635801 %\n",
      "progress:  3.213802435723951 %\n",
      "progress:  3.237966363812101 %\n",
      "progress:  3.262130291900251 %\n",
      "progress:  3.286294219988401 %\n",
      "progress:  3.310458148076551 %\n",
      "progress:  3.334622076164701 %\n",
      "progress:  3.3587860042528512 %\n",
      "progress:  3.3829499323410013 %\n",
      "progress:  3.4071138604291513 %\n",
      "progress:  3.4312777885173014 %\n",
      "progress:  3.4554417166054514 %\n",
      "progress:  3.4796056446936015 %\n",
      "progress:  3.5037695727817515 %\n",
      "progress:  3.5279335008699015 %\n",
      "progress:  3.5520974289580516 %\n",
      "progress:  3.5762613570462016 %\n",
      "progress:  3.6004252851343517 %\n",
      "progress:  3.6245892132225017 %\n",
      "progress:  3.6487531413106518 %\n",
      "progress:  3.6729170693988014 %\n",
      "progress:  3.6970809974869514 %\n",
      "progress:  3.7212449255751014 %\n",
      "progress:  3.7454088536632515 %\n",
      "progress:  3.7695727817514015 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-183ec3bea0b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtokenized_reviews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcleaned_reviews\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtokenized_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_pos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_reviews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtdm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_tdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_reviews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-78383e6b4d64>\u001b[0m in \u001b[0;36mget_tokens\u001b[1;34m(df, stem, negation)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# Now we can get rid of punctuation and also let's fix some spellings:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mtokenized_review\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcorrection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenized_review\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\".\"\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-78383e6b4d64>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# Now we can get rid of punctuation and also let's fix some spellings:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mtokenized_review\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcorrection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenized_review\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\".\"\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-78383e6b4d64>\u001b[0m in \u001b[0;36mcorrection\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;31m#         spell = Speller(lang='en')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python3.7\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36mcorrect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregexp_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mr\"\\w+|[^\\w\\s]|\\s\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m         \u001b[0mcorrected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mWord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrected\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python3.7\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[1;31m# regex matches: word or punctuation or whitespace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregexp_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mr\"\\w+|[^\\w\\s]|\\s\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m         \u001b[0mcorrected\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mWord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrected\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python3.7\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36mcorrect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mversionadded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.6\u001b[0m\u001b[1;36m.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         '''\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mWord\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspellcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcached_property\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python3.7\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36mspellcheck\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mversionadded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.6\u001b[0m\u001b[1;36m.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         '''\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuggest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python3.7\\lib\\site-packages\\textblob\\en\\__init__.py\u001b[0m in \u001b[0;36msuggest\u001b[1;34m(w)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \"\"\" Returns a list of (word, confidence)-tuples of spelling corrections.\n\u001b[0;32m    122\u001b[0m     \"\"\"\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mspelling\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpolarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python3.7\\lib\\site-packages\\textblob\\_text.py\u001b[0m in \u001b[0;36msuggest\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m   1396\u001b[0m         \u001b[0mcandidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_known\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1397\u001b[0m                   \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_known\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1398\u001b[1;33m                   \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_known\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_edit2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1399\u001b[0m                   \u001b[1;32mor\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1400\u001b[0m         \u001b[0mcandidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python3.7\\lib\\site-packages\\textblob\\_text.py\u001b[0m in \u001b[0;36m_edit2\u001b[1;34m(self, w)\u001b[0m\n\u001b[0;32m   1373\u001b[0m         \u001b[1;31m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m         \u001b[1;31m# Only keep candidates that are actually known words (20% speedup).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1375\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0me2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_known\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python3.7\\lib\\site-packages\\textblob\\_text.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1373\u001b[0m         \u001b[1;31m# Of all spelling errors, 99% is covered by edit distance 2.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m         \u001b[1;31m# Only keep candidates that are actually known words (20% speedup).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1375\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_edit1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0me2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_known\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python3.7\\lib\\site-packages\\textblob\\_text.py\u001b[0m in \u001b[0;36m__contains__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__iter__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__contains__\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__getitem__\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python3.7\\lib\\site-packages\\textblob\\_text.py\u001b[0m in \u001b[0;36m_lazy\u001b[1;34m(self, method, *args)\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMethodType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tic=timeit.default_timer()\n",
    "\n",
    "tokenized_reviews = get_tokens(df, stem = False, negation = False)\n",
    "tokenized_pos = get_pos(tokenized_reviews)\n",
    "tdm = get_tdm(tokenized_reviews)\n",
    "vsm = normalize_tdm(tdm)\n",
    "tf_idf = get_tf_idf_transform(tokenized_reviews, tdm, n_reviews)\n",
    "\n",
    "toc=timeit.default_timer()\n",
    "\n",
    "print(\"minutes: \", (toc - tic)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The phone never worked\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['phone', 'never', 'worked']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'phone never worked'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('phone', 'NN'), ('never', 'RB'), ('worked', 'VBD')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'phone': 1, 'never': 1, 'worked': 1})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'phone': 0.9573556625915063,\n",
       "             'never': 5.107803289534515,\n",
       "             'worked': 4.965784284662088})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lookup_review = 3\n",
    "for val in df[df.id_new_col == lookup_review][\"Review\"]: print(val)\n",
    "display(tokenized_reviews[lookup_review])\n",
    "# display(tokenized_pos[lookup_review])\n",
    "# display(tdm[lookup_review])\n",
    "# display(tf_idf[lookup_review])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "from gensim.models import word2vec, KeyedVectors\n",
    "\n",
    "def generate_z1(tokenized_reviews,num_workers = 4, model_name = \"model1_100features_40minwords_5context\"):\n",
    "    \"\"\"Set values for the parameters of the your word2vec model and train it on the extracted sentences from both the train and unlabeled_train data and\n",
    "      generate the collection of these representations Z1.\n",
    "      You should carry out the following.\n",
    "      1) Set the parameters as mentioned below. \n",
    "        A)Constrained Paramters : \"context length\", \"embedding dimension\", \"min_words\" (Please check the question for the values.)\n",
    "        B)Optional Parameters: \"number of workers\", \"downsample setting\"\n",
    "      2) Train your word2vec model and save it.\n",
    "      3) Store the collection of word embeddings and the word_list(z1 and word_list_z1) .\n",
    "\n",
    "      Arg: sentences: List of tokenized sentences (List)\n",
    "            num_features: Word vector dimensionality (int)\n",
    "            min_word_count: Minimum word count (int)\n",
    "            context: Context window size (int)\n",
    "            num_workers: Number of threads to run in parallel (int)\n",
    "            downsampling: Downsample setting for frequent words (float)\n",
    "            model_name = Name to save your model (str)\n",
    "      Returns:\n",
    "            trained_word2vec_model: word2vec model trained on the tokenized sentences.\n",
    "            z1: word embeddings (ndarray)\n",
    "            word_list_z1: List of tokens in the model (List)\n",
    "\n",
    "      \n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Training model...\")\n",
    "    ### Add your code here.\n",
    "    trained_word2vec_model = word2vec.Word2Vec(tokenized_reviews, workers=num_workers)\n",
    "\n",
    "    trained_word2vec_model.init_sims(replace=True)\n",
    "    trained_word2vec_model.save(model_name)\n",
    "    z1 = trained_word2vec_model.wv.vectors\n",
    "    word_list_z1 = trained_word2vec_model.wv.index2word\n",
    "    #######################\n",
    "    return trained_word2vec_model, z1, word_list_z1\n",
    "\n",
    "# Set values for the parameters of the your word2vec model and train it on the extracted sentences from both the train and unlabeled_train data and\n",
    "# generate the collection of these representations Z1.\n",
    "# Add the function parameter values below.\n",
    "### Add your code here.\n",
    "num_features = 10\n",
    "# min_word_count = 40\n",
    "# context = 5\n",
    "######################\n",
    "\n",
    "model1, z1, word_list_z1 = generate_z1(tokenized_reviews,num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "801"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_kmeans(z,word_list_z,num_clusters = 20):\n",
    "    \"\"\" Fit kmeans on the embedding representations and return a mapping of word to cluster indices. Please use the default values for\n",
    "        the rest of the kmeans parameters.\n",
    "\n",
    "        Arg: z: word embeddings (ndarray)\n",
    "              word_list_z: List of tokens in the model (List) \n",
    "              num_clusters: Number of clusters (int)\n",
    "        Returns:\n",
    "            pre_trained_word2vec_model: word2vec model trained on the tokenized sentences.\n",
    "            z2: word embeddings (ndarray)\n",
    "            word_centroid_map_z: A mapping of word to cluster index it belongs to. (Dict)      \n",
    "\n",
    "    \"\"\"\n",
    "    ### Add your code here.\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    z_cluster_result = kmeans.fit_predict(z)\n",
    "    n = len(word_list_z)\n",
    "    word_centroid_map_z = {}\n",
    "    for i in range(n):\n",
    "        word_centroid_map_z[word_list_z[i]] = z_cluster_result[i] \n",
    "    ######################\n",
    "\n",
    "    return word_centroid_map_z\n",
    "\n",
    "\n",
    "word_centroid_map_z1 = fit_kmeans(z1, word_list_z1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The clusters for model1 are....\n",
      "\n",
      "Cluster 1\n",
      "['excelente', 'charges', 'updated', 'randomly', 'updates', 'contract']\n",
      "\n",
      "Cluster 2\n",
      "['waiting']\n",
      "\n",
      "Cluster 3\n",
      "['freeze', 'mind', 'parents', 'cracked']\n",
      "\n",
      "Cluster 4\n",
      "['seconds']\n",
      "\n",
      "Cluster 5\n",
      "['wanted', 'model', 'hand', 'color', 'exactly', 'looking', 'fake', 'cool', 'number', 'cover', 'maps', 'live', 'kept', 'maybe', 'gets', 'mode', 'access', 'difference', 'cable', 'says']\n",
      "\n",
      "Cluster 6\n",
      "['language']\n",
      "\n",
      "Cluster 7\n",
      "['birthday']\n",
      "\n",
      "Cluster 8\n",
      "['clot']\n",
      "\n",
      "Cluster 9\n",
      "['huge']\n",
      "\n",
      "Cluster 10\n",
      "['drop']\n",
      "\n",
      "Cluster 11\n",
      "['cel']\n",
      "\n",
      "Cluster 12\n",
      "['reception']\n",
      "\n",
      "Cluster 13\n",
      "['wonderful']\n",
      "\n",
      "Cluster 14\n",
      "['high', 'anymore', 'died', 'reset', 'connected', 'turned', 'longer', 'complete', 'lasts', 'figure', 'functions', 'friends', 'anyway', 'mhz', 'faster', 'impressive', 'budget', '.for', 'bright', 'standard']\n",
      "\n",
      "Cluster 15\n",
      "['phone', 'good', 'great', 'like', 'one', 'battery', 'would', 'camera', 'screen', 'use', 'get', 'works', 'love', 'new', 'excellent', 'work', 'price', 'buy', 'dont', 'well']\n",
      "\n",
      "Cluster 16\n",
      "['stuff']\n",
      "\n",
      "Cluster 17\n",
      "['opened']\n",
      "\n",
      "Cluster 18\n",
      "['available', 'fix', 'remove', 'dracone', 'meet', 'fact', 'husband', 'id', 'phones', 'star', 'suppose', 'helps', 'usage', 'actual', 'packaging', 'initially', 'stated', 'shut', 'stopped', 'others']\n",
      "\n",
      "Cluster 19\n",
      "['old', 'keep', 'thanks', 'worth', 'experience', 'advertised', 'disappointed', 'broke', 'absolutely', 'stop', 'deal', 'receive', 'half', 'decent', 'returned', 'actually', 'devices', 'ill', 'scratches', 'activated']\n",
      "\n",
      "Cluster 20\n",
      "['mom']\n"
     ]
    }
   ],
   "source": [
    "def print_clusters(word_centroid_map_z, model_name):\n",
    "    \"\"\" Print min(20, cluster_size) words for each of the clusters.\n",
    "\n",
    "        Args: word_centroid_map_z: A mapping of word to cluster index it belongs to. (Dict)  \n",
    "              model_name: Model Name (str)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"The clusters for {0} are....\".format(model_name))\n",
    "    ### Add your code here.\n",
    "    for i in range(20):\n",
    "        cluster = i+1\n",
    "        print(\"\\nCluster %d\" % cluster)\n",
    "        \n",
    "        words = []\n",
    "        count = 0\n",
    "        for key, value in word_centroid_map_z.items():\n",
    "            if count == 20:\n",
    "                    break\n",
    "            if value == i:\n",
    "                words.append(key)\n",
    "                count += 1\n",
    "        print (words)\n",
    "    ######################\n",
    "print_clusters(word_centroid_map_z1, \"model1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bag_of_centroids(tokenized_reviews, word_centroid_map, num_clusters = 20):\n",
    "    \"\"\" Create a bag of kmeans centroids for each review i.e. for a review we return an array of length num_clusters with each \n",
    "        element of the array indicating how many words(tokens) of the review belong to that cluster.\n",
    "\n",
    "        Args: tokenized_review: list of tokens corresponding to a review (List)\n",
    "              word_centroid_map: word to cluster_index map for the model (Dict)\n",
    "              num_clusters: Number of clusters (int)\n",
    "        \n",
    "        Returns: bag_of_centroids: An array containing the count of tokens in each cluster (ndarray)\n",
    "    \"\"\"\n",
    "  \n",
    "    ### Add your code here.\n",
    "    bag_of_centroids = np.zeros(num_clusters)\n",
    "    n = len(tokenized_reviews)\n",
    "    for word in tokenized_reviews:\n",
    "        if word in word_centroid_map:\n",
    "            index = word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    #######################\n",
    "    return bag_of_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_design_matrices(data, tokenized_reviews, word_centroid_map_z1, num_clusters = 20):\n",
    "    \"\"\" Creates the design matrices X1(for trained model) and X2(for pretrained google word2vec model) for the given data\n",
    "\n",
    "        Args: data: Train/Test data (pandas.core.frame.DataFrame)\n",
    "              cleaned_reviews: List of tokenized reviews(sentences are not split and stopwords removed) (List)\n",
    "              word_centroid_map_z1: word to cluster map for model1 (Dict)\n",
    "              word_centroid_map_z2: word to cluster map for model2 (Dict)\n",
    "              num_clusters: Number of KMeans clusters (int)\n",
    "\n",
    "        Returns:\n",
    "              x1_data: Design matrices X1-- Shape should be num_reviews*num_clusters (np.ndarray) \n",
    "              x2_data: Design matrices X2-- Shape should be num_reviews*num_clusters (np.ndarray)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ### Add your code here.\n",
    "    x1_data = np.zeros((df[\"Review\"].size, num_clusters))\n",
    "\n",
    "    for i in range(len(tokenized_reviews)):\n",
    "        x1_data[i] = create_bag_of_centroids(tokenized_reviews[i], word_centroid_map_z1, num_clusters = 20)\n",
    "    ######################\n",
    "    return x1_data\n",
    "x1_data = create_design_matrices(df,tokenized_reviews,word_centroid_map_z1,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**x1_data** is the data we are going to pass in for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_tokens(df):\n",
    "    stop = set(stopwords.words('english'))\n",
    "    products = []\n",
    "    i = 1\n",
    "    \n",
    "    for product in df[\"Product\"]:\n",
    "        tokenized_product = []      \n",
    "\n",
    "        product = product.lower() # lowercase\n",
    "        \n",
    "        # Remove every character except A-Z, a-z,space \n",
    "        # and punctuation (we'll need it for negation)\n",
    "        product = re.sub(r'[^0-9A-Za-z \\.]','',product)    \n",
    "    \n",
    "        # Only consider first 10 words of the product names\n",
    "        tokens = word_tokenize(product)[:11]\n",
    "        \n",
    "        for token in tokens:\n",
    "            # Remove stop words\n",
    "            if token not in stop:\n",
    "                tokenized_product.append(token)       \n",
    "            \n",
    "        products.append(tokenized_product)\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print('progress: ', (i/len(df[\"Product\"]))*100, \"%\")\n",
    "        i = i + 1\n",
    "        \n",
    "    return products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress:  10.0 %\n",
      "progress:  20.0 %\n",
      "progress:  30.0 %\n",
      "progress:  40.0 %\n",
      "progress:  50.0 %\n",
      "progress:  60.0 %\n",
      "progress:  70.0 %\n",
      "progress:  80.0 %\n",
      "progress:  90.0 %\n",
      "progress:  100.0 %\n"
     ]
    }
   ],
   "source": [
    "tokenized_products = get_product_tokens(df)\n",
    "products_tokenized_pos = get_pos(tokenized_products)\n",
    "products_tdm = get_tdm(tokenized_products)\n",
    "products_tf_idf = get_tf_idf_transform(tokenized_products, products_tdm, n_reviews)\n",
    "products_idf = get_idf_transform(tokenized_products, products_tdm, n_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18416    Apple iPhone 5 16GB - Unlocked - Black (Certif...\n",
       "Name: Product, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('refurbished', 5.442222328605075),\n",
       " ('5', 4.608232280044003),\n",
       " ('certified', 4.50635266602479),\n",
       " ('black', 2.7563309190331373),\n",
       " ('16gb', 2.6529013293777317),\n",
       " ('apple', 2.2378638300988882),\n",
       " ('iphone', 2.1778817252706553),\n",
       " ('unlocked', 0.5104570643575262)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('apple', 'NN'),\n",
       " ('iphone', 'NN'),\n",
       " ('5', 'CD'),\n",
       " ('16gb', 'CD'),\n",
       " ('unlocked', 'JJ'),\n",
       " ('black', 'JJ'),\n",
       " ('certified', 'VBN'),\n",
       " ('refurbished', 'VBD')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lookup_product = 53\n",
    "display(df[df.id_new_col== lookup_product][\"Product\"])\n",
    "\n",
    "# we want to grab those with higher scores (least common terms)\n",
    "display(sorted(products_idf[lookup_product].items(), \n",
    "               key=operator.itemgetter(1), reverse = True)) \n",
    "\n",
    "# Unfortunately we can't filter through POS\n",
    "display(products_tokenized_pos[lookup_product])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"black\", \"red\", \"blue\", \"white\", \"gray\", \"green\",\"yellow\", \"pink\", \"gold\"]\n",
    "common_terms = [\"smarthphone\", \"phone\", \"cellphone\", \"retail\", \"warranty\", \n",
    "                \"silver\", \"bluetooth\", \"wifi\", \"wireless\", \"keyboard\", \"gps\",\n",
    "               \"original\", \"unlocked\", \"camera\", \"certified\", \"international\",\n",
    "               \"actory\", \"packaging\", \"us\", \"usa\", \"international\", \"refurbished\", \n",
    "               \"phones\", \"att\", \"verizon\", \"-\", \"8gb\", \"16gb\", \"32gb\", \"64gb\", \"contract\"]\n",
    "\n",
    "def standardize_names(products_idf, colors, common_terms):\n",
    "    standard_names = []\n",
    "    brands = [str(x).lower() for x in set(df.Brand)]\n",
    "    \n",
    "    for product in products_idf:\n",
    "        \n",
    "        for k, v in product.items():\n",
    "            # Remove color and brand words\n",
    "            if k in colors or k in common_terms or k in brands:\n",
    "                product[k] = 0\n",
    "        \n",
    "        # Grab the first 5 words with highest score\n",
    "        product = sorted(product.items(), key=operator.itemgetter(1), reverse = True)[:5]\n",
    "        \n",
    "        standard_names.append(product)\n",
    "        \n",
    "        tokenized_standard_product_names = []\n",
    "        \n",
    "    for product in standard_names:\n",
    "        product_name = []\n",
    "        for word in product:\n",
    "            if word[1] > 0:\n",
    "                product_name.append(word[0])\n",
    "\n",
    "        tokenized_standard_product_names.append(product_name)\n",
    "    \n",
    "    \n",
    "        \n",
    "    return tokenized_standard_product_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'mt6582m': 0.4472135954999579,\n",
       "             '1331mhz': 0.4472135954999579,\n",
       "             'a850': 0.4472135954999579,\n",
       "             'smartphone960x540': 0.4472135954999579,\n",
       "             '5.5inch': 0.4472135954999579})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_product_names = standardize_names(products_idf, colors, common_terms)\n",
    "\n",
    "product_tdm = get_tdm(standard_product_names)\n",
    "product_vsm = normalize_tdm(product_tdm)\n",
    "product_vsm[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>curve</th>\n",
       "      <th>intuition</th>\n",
       "      <th>s4</th>\n",
       "      <th>large</th>\n",
       "      <th>version</th>\n",
       "      <th>6.0</th>\n",
       "      <th>4.0k</th>\n",
       "      <th>focus</th>\n",
       "      <th>space</th>\n",
       "      <th>milspec</th>\n",
       "      <th>...</th>\n",
       "      <th>cam</th>\n",
       "      <th>rogue</th>\n",
       "      <th>12mp</th>\n",
       "      <th>e5823</th>\n",
       "      <th>english</th>\n",
       "      <th>android</th>\n",
       "      <th>g3</th>\n",
       "      <th>9780</th>\n",
       "      <th>product_name_cluster</th>\n",
       "      <th>id_col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>394</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 872 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   curve  intuition  s4  large  version  6.0  4.0k  focus  space  milspec  \\\n",
       "0      0          0   0      0        0    0     0      0      0        0   \n",
       "1      0          0   0      0        0    0     0      0      0        0   \n",
       "2      0          0   0      0        0    0     0      0      0        0   \n",
       "3      0          0   0      0        0    0     0      0      0        0   \n",
       "4      0          0   0      0        0    0     0      0      0        0   \n",
       "\n",
       "   ...  cam  rogue  12mp  e5823  english  android  g3  9780  \\\n",
       "0  ...    0      0     0      0        0        0   0     0   \n",
       "1  ...    0      0     0      0        0        0   0     0   \n",
       "2  ...    0      0     0      0        0        0   0     0   \n",
       "3  ...    0      0     0      0        0        0   0     0   \n",
       "4  ...    0      0     0      0        0        0   0     0   \n",
       "\n",
       "   product_name_cluster  id_col  \n",
       "0                    48       0  \n",
       "1                    48       1  \n",
       "2                   200       2  \n",
       "3                    97       3  \n",
       "4                   394       4  \n",
       "\n",
       "[5 rows x 872 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_name_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    product_name_cluster\n",
       "16                    31\n",
       "22                    17\n",
       "25                    15\n",
       "51                    12\n",
       "20                    12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "similarity = product_tdm\n",
    "product_names_clusters = int(round(n_reviews/2,0))\n",
    "\n",
    "similarity_matrix = pd.DataFrame(get_similarity_matrix(similarity, standard_product_names), columns = get_all_terms(standard_product_names))\n",
    "\n",
    "kmeans = KMeans(n_clusters=product_names_clusters, random_state=0).fit(similarity_matrix)\n",
    "clusters=kmeans.labels_.tolist()\n",
    "\n",
    "clustered_matrix = similarity_matrix.copy()\n",
    "clustered_matrix['product_name_cluster'] = clusters\n",
    "clustered_matrix['id_col'] = range(0, n_reviews)\n",
    "\n",
    "display(clustered_matrix[:5])\n",
    "\n",
    "count_clusters = pd.DataFrame(clustered_matrix.product_name_cluster.value_counts())\n",
    "display(count_clusters[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>Votes</th>\n",
       "      <th>id_col</th>\n",
       "      <th>Label</th>\n",
       "      <th>id_new_col</th>\n",
       "      <th>cluster_name</th>\n",
       "      <th>Standard_Product_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>5.5-Inch Unlocked Lenovo A850 3G Smartphone-(9...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>161.06</td>\n",
       "      <td>5</td>\n",
       "      <td>exelente</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1093</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>5.5-Inch Lenovo A850 3G Smartphone-(960x540) Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>5.5-Inch Unlocked Lenovo A850 3G Smartphone-(9...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>161.06</td>\n",
       "      <td>5</td>\n",
       "      <td>Exelente</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1164</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>5.5-Inch Lenovo A850 3G Smartphone-(960x540) Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>6\" Inch Unlocked Android 4.4.2 MTK6572 Dual Co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.00</td>\n",
       "      <td>1</td>\n",
       "      <td>The phone never worked</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1342</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>6\" Inch Android 4.4.2 MTK6572 Dual Core Smartp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>ALCATEL OneTouch Idol 3 Global Unlocked 4G LTE...</td>\n",
       "      <td>Alcatel</td>\n",
       "      <td>292.98</td>\n",
       "      <td>5</td>\n",
       "      <td>Iv'e had the phone for around a week now and j...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1899</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>97</td>\n",
       "      <td>ALCATEL OneTouch Idol 3 Global 4G LTE Smartpho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>ALCATEL OneTouch Idol 3 Global Unlocked 4G LTE...</td>\n",
       "      <td>Alcatel</td>\n",
       "      <td>129.00</td>\n",
       "      <td>1</td>\n",
       "      <td>Stay away from Alcatel products. They do not d...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2196</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>394</td>\n",
       "      <td>ALCATEL OneTouch Idol 3 Global 4G LTE Smartpho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Product    Brand   Price  \\\n",
       "1093  5.5-Inch Unlocked Lenovo A850 3G Smartphone-(9...      NaN  161.06   \n",
       "1164  5.5-Inch Unlocked Lenovo A850 3G Smartphone-(9...      NaN  161.06   \n",
       "1342  6\" Inch Unlocked Android 4.4.2 MTK6572 Dual Co...      NaN  100.00   \n",
       "1899  ALCATEL OneTouch Idol 3 Global Unlocked 4G LTE...  Alcatel  292.98   \n",
       "2196  ALCATEL OneTouch Idol 3 Global Unlocked 4G LTE...  Alcatel  129.00   \n",
       "\n",
       "      Rating                                             Review  Votes  \\\n",
       "1093       5                                           exelente    0.0   \n",
       "1164       5                                           Exelente    0.0   \n",
       "1342       1                             The phone never worked    1.0   \n",
       "1899       5  Iv'e had the phone for around a week now and j...    6.0   \n",
       "2196       1  Stay away from Alcatel products. They do not d...    1.0   \n",
       "\n",
       "      id_col  Label  id_new_col  cluster_name  \\\n",
       "1093    1093      1           0            48   \n",
       "1164    1164      1           1            48   \n",
       "1342    1342     -1           2           200   \n",
       "1899    1899      1           3            97   \n",
       "2196    2196     -1           4           394   \n",
       "\n",
       "                                  Standard_Product_Name  \n",
       "1093  5.5-Inch Lenovo A850 3G Smartphone-(960x540) Q...  \n",
       "1164  5.5-Inch Lenovo A850 3G Smartphone-(960x540) Q...  \n",
       "1342  6\" Inch Android 4.4.2 MTK6572 Dual Core Smartp...  \n",
       "1899  ALCATEL OneTouch Idol 3 Global 4G LTE Smartpho...  \n",
       "2196  ALCATEL OneTouch Idol 3 Global 4G LTE Smartpho...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cluster_name\"] = list(clustered_matrix.product_name_cluster)\n",
    "\n",
    "def create_standard_name(df):\n",
    "    new_names = defaultdict(int)\n",
    "    \n",
    "    current_names = df.groupby('cluster_name').first().Product\n",
    "    \n",
    "    \n",
    "    for i in set(clusters):\n",
    "        cluster_name = df[df.cluster_name == i].Product.value_counts().index[0]\n",
    "        new_name = []\n",
    "        \n",
    "        for word in cluster_name.split():\n",
    "            temp_word= re.sub(r'[^0-9A-Za-z \\.\\-]','',word).lower()\n",
    "            if temp_word not in colors and temp_word not in common_terms :\n",
    "                new_name.append(word)\n",
    "        new_names[i] = ' '.join(new_name)\n",
    "    \n",
    "    new_standard_names = []\n",
    "    \n",
    "\n",
    "    for row in df.cluster_name:\n",
    "        \n",
    "        new_standard_names.append(new_names[row])\n",
    "    \n",
    "    df[\"Standard_Product_Name\"] = new_standard_names\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = create_standard_name(df)         \n",
    "        \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
